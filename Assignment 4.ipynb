{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: Yajur Vashisht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset (1 mark)\n",
    "\n",
    "df = pd.read_csv('salaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. My dataset is from: https://www.kaggle.com/datasets/infosecjobs/global-salaries-in-cybersecurity-infosec/data\n",
    "2. I think this is a relevant dataset as many people want to see what their salaries would look like or what a good range is for them depending on their experience level and the size of the company they are working at. \n",
    "3. There are a few challenges with finding relevant data. I liked that the data in this dataset was very current (2020-2023) because there has been record inflation and if I wa using historical data then it could skew the results. I also like that the data is transformed to salary in usd to give a constant to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>EX</td>\n",
       "      <td>FT</td>\n",
       "      <td>Information Security Officer</td>\n",
       "      <td>160000</td>\n",
       "      <td>USD</td>\n",
       "      <td>160000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>EX</td>\n",
       "      <td>FT</td>\n",
       "      <td>Information Security Officer</td>\n",
       "      <td>100000</td>\n",
       "      <td>USD</td>\n",
       "      <td>100000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>247250</td>\n",
       "      <td>USD</td>\n",
       "      <td>247250</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>160000</td>\n",
       "      <td>USD</td>\n",
       "      <td>160000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Security Engineer</td>\n",
       "      <td>224250</td>\n",
       "      <td>USD</td>\n",
       "      <td>224250</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type                     job_title  \\\n",
       "0       2023               EX              FT  Information Security Officer   \n",
       "1       2023               EX              FT  Information Security Officer   \n",
       "2       2023               SE              FT             Security Engineer   \n",
       "3       2023               SE              FT             Security Engineer   \n",
       "4       2023               SE              FT             Security Engineer   \n",
       "\n",
       "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0  160000             USD         160000                 US           100   \n",
       "1  100000             USD         100000                 US           100   \n",
       "2  247250             USD         247250                 US             0   \n",
       "3  160000             USD         160000                 US             0   \n",
       "4  224250             USD         224250                 US             0   \n",
       "\n",
       "  company_location company_size  \n",
       "0               US            M  \n",
       "1               US            M  \n",
       "2               US            M  \n",
       "3               US            M  \n",
       "4               US            M  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc9afb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work_year              int64\n",
       "experience_level      object\n",
       "employment_type       object\n",
       "job_title             object\n",
       "salary                 int64\n",
       "salary_currency       object\n",
       "salary_in_usd          int64\n",
       "employee_residence    object\n",
       "remote_ratio           int64\n",
       "company_location      object\n",
       "company_size          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0f2497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work_year             0\n",
      "experience_level      0\n",
      "employment_type       0\n",
      "job_title             0\n",
      "salary                0\n",
      "salary_currency       0\n",
      "salary_in_usd         0\n",
      "employee_residence    0\n",
      "remote_ratio          0\n",
      "company_location      0\n",
      "company_size          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_values_per_column = df.isnull().sum()\n",
    "print(null_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56af1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level_EN</th>\n",
       "      <th>experience_level_EX</th>\n",
       "      <th>experience_level_MI</th>\n",
       "      <th>experience_level_SE</th>\n",
       "      <th>company_size_L</th>\n",
       "      <th>company_size_M</th>\n",
       "      <th>company_size_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      experience_level_EN  experience_level_EX  experience_level_MI  \\\n",
       "1173                  0.0                  0.0                  1.0   \n",
       "168                   0.0                  0.0                  0.0   \n",
       "2589                  1.0                  0.0                  0.0   \n",
       "1011                  0.0                  0.0                  0.0   \n",
       "2977                  0.0                  0.0                  1.0   \n",
       "\n",
       "      experience_level_SE  company_size_L  company_size_M  company_size_S  \n",
       "1173                  0.0             1.0             0.0             0.0  \n",
       "168                   1.0             0.0             1.0             0.0  \n",
       "2589                  0.0             1.0             0.0             0.0  \n",
       "1011                  1.0             0.0             1.0             0.0  \n",
       "2977                  0.0             1.0             0.0             0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform data\n",
    "encoded_data = encoder.fit_transform(df[['experience_level', 'company_size']])\n",
    "encoded_columns = encoder.get_feature_names_out(['experience_level', 'company_size'])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)\n",
    "\n",
    "# Define features and target\n",
    "y = df['salary_in_usd']\n",
    "X = encoded_df\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. There were no missing and null values in my dataset. If there was I would either replace them with 0, fill with mean, or drop the row entirely. \n",
    "\n",
    "2. I have mostly numerical data, salary_in_usd will be my y but I needed to Encode the experience_level columns and the company_size columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Best parameters for Linear Regression: {}\n",
      "Best score for Linear Regression: -3157074833.2540436\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
      "Best score for Random Forest: -3158408973.1292124\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best score for Gradient Boosting: -3158569672.567332\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grid_lr = {}  # Linear Regression may not have many hyperparameters to tune\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Set up grid search for each model\n",
    "grid_search_lr = GridSearchCV(LinearRegression(), param_grid_lr, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit models and print the best parameters and scores\n",
    "# Fit Linear Regression\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "print(\"Best parameters for Linear Regression:\", grid_search_lr.best_params_)\n",
    "print(\"Best score for Linear Regression:\", grid_search_lr.best_score_)\n",
    "\n",
    "# Fit Random Forest\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best score for Random Forest:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Fit Gradient Boosting\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "print(\"Best score for Gradient Boosting:\", grid_search_gb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. I need a regression model. This is because my target variable, salary_in_usd, is a continuous numeric variable. Regression models are used when the output variable is a real or continuous value.\n",
    "\n",
    "2. I used three different models. Linear Regression: It is the simplest regression model and is a good baseline. It works well when there is a linear relationship between the features and the target variable. Random Forest Regressor: A model that uses an ensemble of decision trees. It was chosen for its ability to handle complex datasets with higher dimensionality and its robustness to overfitting. Gradient Boosting Regressor: Works by building trees in a sequential manner, where each tree tries to correct the errors of the previous one. It's good for high performance in a variety of tasks and its ability to handle heterogeneous features.\n",
    "\n",
    "3. Based on the results, the Linear Regression model worked the best. This is indicated by its lowest negative score in the grid search. Linear Regression is less prone to overfitting compared to more complex models like Random Forest and Gradient Boosting, especially if the dataset isn't very large or if the features don't have complex non-linear relationships with the target variable. Since some of the trickier variables were dropped because I wanted to compare exeperience level and company size vs. the salary an employee recieves, it makes sense that linear worked the best. In the context of salary prediction, many factors could have a linear impact on salaries like years of experience, level of education, etc. If the relationships in the data are not highly non-linear, simpler models like Linear Regression can often yield better results, as we have seen demonstrated in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 Results: 56187.853075678584\n",
      "Testing set MSE: 3233701947.4403224\n",
      "Testing set RMSE: 56865.64821964419\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the best model with the best parameters\n",
    "best_model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Fit the model on the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "# Calculate the Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "part_3_result = np.sqrt(-1*-3157074833.2540436)\n",
    "print(f\"Part 3 Results: {part_3_result}\")\n",
    "print(f\"Testing set MSE: {mse}\")\n",
    "print(f\"Testing set RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f68f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between both RSME:  -677.7951439656026\n"
     ]
    }
   ],
   "source": [
    "difference_rsme = part_3_result - rmse\n",
    "print(\"The difference between both RSME: \", difference_rsme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380ff0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max salary in USD is:  456621\n",
      "The min salary in USD is:  15897\n",
      "The standard deviation of the salaries in USD is:  64924.26332277093\n"
     ]
    }
   ],
   "source": [
    "max = df['salary_in_usd'].max()\n",
    "min = df['salary_in_usd'].min()\n",
    "print(\"The max salary in USD is: \", max)\n",
    "print(\"The min salary in USD is: \", min)\n",
    "Stdev = df['salary_in_usd'].std() \n",
    "print(\"The standard deviation of the salaries in USD is: \", Stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. I chose RSME as my accuracy metric. This is because the salary_in_usd column has massive variations in data, from as little as 16,000 USD to 456,621 USD.\n",
    "2. Going from Part 3 to Part 4, the training vs testing set produced a very similar RSME (a differenc of 677) which in the context of 100,000's is negligible. \n",
    "3. Based on the results and context of my dataset there is absolutely no way the model performed well enough to be used in a real world application. An RSME of 56866 is massive in the context of a salary. I believe a major part of this is because I had to remove the Job Title because using HotEncoder() on it caused there to be way too many columns which was unproductive since some of the job titles were extremly similar. A suggestion would be to group like jobs together so when encoding is done it will not cause the dataset to be split into an additional 20 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "1. My code was sourced from the Pipeline and ColumnTransformer examples that we went over in class along with the regression metrics examples we did. I also used a lot of the previous assignment to reference how to set up a lot of the same steps that we do every time we do machine learning.\n",
    "\n",
    "2. The steps were completed in the order they were presented in the assignment itself. Unless the pro-processing is complete the rest of the steps cannot be completed. For parts that I got stuck on I referenced our notes until I was able to work through all of the problems sequentially. I also find for machine learning since the five steps are always the same it is just easier to use the same methodology and go from step one to five in order.\n",
    "\n",
    "3. Generative AI was used for some of the pre-processing methods because it was a nightmare to get this dataset cleaned up enough to do work on. The prompts include: \n",
    "\n",
    "- What's the difference between: make_column_transformer and this: ColumnTransformer\n",
    "- Invalid parameter 'normalize' for estimator LinearRegression(). Valid parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive'].\n",
    "- There's no way this is right: \n",
    "Linear Regression Best Params: {'fit_intercept': False}\n",
    "Linear Regression Best Score: -3203022745.01006\n",
    "Random Forest Best Params: {'max_depth': 10, 'n_estimators': 200}\n",
    "Random Forest Best Score: -3205715167.8523016\n",
    "Gradient Boosting Best Params: {'learning_rate': 0.1, 'n_estimators': 100}\n",
    "Gradient Boosting Best Score: -3205302866.120061\n",
    "\n",
    "The use of GenerativeAI was for debugging and ensuring I was not going crazy due to the massive MSE's I got. The code was not used, it was more so used for checking that I had inputted everything correctly and I was not missing something causing these huge issues. It turned out all the models were just very poor which was what was causing the issue. That and I believe not using \n",
    "\n",
    "4. I think there were significant challenges in setting up the data and ensuring it was done correctly to minimize the errors produced by the model. Unlike the datasets used in class this is a real dataset which had a lot more challenges for pre-processing. Following the in class examples and trying to take each pre-processing step one at a time helped me for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "The part of the assignment that I really enjoyed was using different models and seeing how they interact with data differently. I disliked also using those multiple models because some of the lines of code started to look the same after a while and it was really hard to differentiate. What I found very interesting about this was being able to use data sets that are seemingly mundane an create interesting models and observations via the models to learn more about what seems like very boring data. I found it challenging working with so many pre-processing methods to clean my data instead of having a relatively cleaned dataset to begin with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
