{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Yajur Vashisht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8)\n",
      "(1030,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "print(X.shape)\n",
    "X.dtypes\n",
    "print(y.shape)\n",
    "y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f0ace59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training shape:  (824, 8)\n",
      "X testing shape:  (206, 8)\n",
      "y training size:  (824,)\n",
      "y testing size:  (206,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"X training shape: \", X_train.shape)\n",
    "print(\"X testing shape: \", X_test.shape)\n",
    "print(\"y training size: \", y_train.shape)\n",
    "print(\"y testing size: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "589cde55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing decision tree, random forest, and gradient boosting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Instantiate Decision Tree with max_depth=5\n",
    "decision_tree_model = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Instantiate Random Forest with max_depth=5\n",
    "# Additional parameters include n_estimators\n",
    "random_forest_model = RandomForestRegressor(max_depth=5, random_state=0, max_features='sqrt', n_estimators=100)  \n",
    "\n",
    "# Instantiate Gradient Boosting with max_depth=5\n",
    "# Additional parameters include n_estimators\n",
    "gradient_boosting_model = GradientBoostingRegressor(max_depth=5, random_state = 0, n_estimators=100)\n",
    "\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "gradient_boosting_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e32963ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average training and validation accurary with MSE and cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Decision Tree \n",
    "decision_tree_score = cross_validate(decision_tree_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# Random Forest \n",
    "random_forest_score = cross_validate(random_forest_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# Gradient Boosting \n",
    "gradient_boosting_score = cross_validate(gradient_boosting_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Accuracy  Validation Accuracy\n",
      "DT             47.823               74.045\n",
      "RF             49.159               67.723\n",
      "GB              3.694               23.547\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "results = pd.DataFrame({\n",
    "    'Training Accuracy': [-decision_tree_score['train_score'].mean(), -random_forest_score['train_score'].mean(), -gradient_boosting_score['train_score'].mean()], \n",
    "                        'Validation Accuracy': [-decision_tree_score['test_score'].mean(), -random_forest_score['test_score'].mean(), -gradient_boosting_score['test_score'].mean()]}\n",
    ", index=['DT', 'RF', 'GB'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83539f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# Decision Tree \n",
    "decision_tree_r2 = cross_validate(decision_tree_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "\n",
    "# Random Forest \n",
    "random_forest_r2 = cross_validate(random_forest_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "\n",
    "# Gradient Boosting \n",
    "gradient_boosting_r2 = cross_validate(gradient_boosting_model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6334afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Accuracy  Validation Accuracy\n",
      "DT           0.830437             0.735184\n",
      "RF           0.825750             0.758736\n",
      "GB           0.986903             0.916155\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.float_format = None\n",
    "results_r2 = pd.DataFrame({\n",
    "    \"Training Accuracy\": [decision_tree_r2['train_score'].mean(), random_forest_r2['train_score'].mean(), gradient_boosting_r2['train_score'].mean()],\n",
    "    \"Validation Accuracy\": [decision_tree_r2['test_score'].mean(), random_forest_r2['test_score'].mean(), gradient_boosting_r2['test_score'].mean()]\n",
    "}, index=['DT', 'RF', 'GB'])\n",
    "\n",
    "print(results_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee689a78",
   "metadata": {},
   "source": [
    "1. The results in this assignment with a Decision Tree, Random Forest, and Gradient Boosting models differ from the previous lab in various ways such as: \n",
    "\n",
    "    - For Mean Squared Error (MSE), all three tree-based models (DT, RF, GB) outperform the linear regression model. This means that they exhibit lower errors in predicting the target variable. This indicates better performance for minimizing prediction error.\n",
    "    - For R2 score, the Gradient Booosting model performs well, with an R2 score of 91.62. However, it might be overfitting, due to the huge discrepancy between training and validation accuracy. The Random Forest and Decision Tree models have a better balance between training and validation accuracy.\n",
    "\n",
    "2. The model I would select for the concrete dataset would be the Gradient Boosting model because compared to the other models used it provides a good training accuracy and crucially a good validation accuracy. This demonstrates the model is not overfitting and is adequately equipped to predict values.\n",
    "\n",
    "3. To increasee the accuracy of the tree-based models used in the assignment we can:\n",
    "\n",
    "- The dataset can be pre-pruned by selecting the max_depth and max_leaf_nodes. \n",
    "- The dataset can also be pre-vetted to ensure outliers have been removed, allowing for more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cc058",
   "metadata": {},
   "source": [
    "1. My code was sourced from the linear regression and linear classification example that we went over in class along with the regression metrics example we did. All the steps were similar.\n",
    "\n",
    "2. The steps were completed in the order they were presented in the assignment itself. For parts that I got stuck on I moved on to the second problem and vice versa until I was able to work through all of the problems.\n",
    "\n",
    "3. Generative AI was not used for any coding itself I used it for some of the error codes I was getting and I asked it if I was categorizing my models correctly because I kept confusing them since we were working with three at a time in the first exercise.\n",
    "\n",
    "4. I don't think there were any real challenges, with machine learning the steps are always the same. Only the interpretation of the models that we will be using changes but the coding itself is straightforward. I find that the coding is not difficult, it's the interpretation of the models and the stats the models produce are difficult to keep straight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (178, 13)\n",
      "X type: Alcohol                         float64\n",
      "Malicacid                       float64\n",
      "Ash                             float64\n",
      "Alcalinity_of_ash               float64\n",
      "Magnesium                         int64\n",
      "Total_phenols                   float64\n",
      "Flavanoids                      float64\n",
      "Nonflavanoid_phenols            float64\n",
      "Proanthocyanins                 float64\n",
      "Color_intensity                 float64\n",
      "Hue                             float64\n",
      "0D280_0D315_of_diluted_wines    float64\n",
      "Proline                           int64\n",
      "dtype: object\n",
      "y shape: (178,)\n",
      "y type: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "\n",
    "# Download the Wine dataset from UCI and define column headers\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "column_headers = [\"class\", \"Alcohol\", \"Malicacid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\", \"Total_phenols\",\n",
    "                  \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\",\n",
    "                  \"0D280_0D315_of_diluted_wines\", \"Proline\"]\n",
    "wine_df = pd.read_csv(url, names=column_headers)\n",
    "\n",
    "# Split the dataset into X and y\n",
    "X = wine_df.drop(\"class\", axis=1)\n",
    "y = wine_df[\"class\"]\n",
    "\n",
    "# Print the size and type of X and y\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X type:\", X.dtypes)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y type:\", y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X first 5 rows: \n",
      "    Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
      "0    14.23       1.71  2.43               15.6        127           2.80   \n",
      "1    13.20       1.78  2.14               11.2        100           2.65   \n",
      "2    13.16       2.36  2.67               18.6        101           2.80   \n",
      "3    14.37       1.95  2.50               16.8        113           3.85   \n",
      "4    13.24       2.59  2.87               21.0        118           2.80   \n",
      "\n",
      "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   0D280_0D315_of_diluted_wines  Proline  \n",
      "0                          3.92     1065  \n",
      "1                          3.40     1050  \n",
      "2                          3.17     1185  \n",
      "3                          3.45     1480  \n",
      "4                          2.93      735  \n",
      "\n",
      "y first 5 rows: \n",
      " 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "print(\"X first 5 rows: \\n\", X.head(5))\n",
    "print(\"\\ny first 5 rows: \\n\", y.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing X values: \n",
      " Alcohol                         0\n",
      "Malicacid                       0\n",
      "Ash                             0\n",
      "Alcalinity_of_ash               0\n",
      "Magnesium                       0\n",
      "Total_phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid_phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color_intensity                 0\n",
      "Hue                             0\n",
      "0D280_0D315_of_diluted_wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n",
      "Number of missing y values: \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "missing_x_values = X.isna().sum()\n",
    "missing_y_values = y.isna().sum()\n",
    "\n",
    "\n",
    "print(\"Number of missing X values: \\n\", missing_x_values)\n",
    "print(\"Number of missing y values: \\n\", missing_y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b37a6fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number samples we have of each type of wine are: \n",
      " 2    71\n",
      "1    59\n",
      "3    48\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "wine_counts = y.value_counts()\n",
    "print(\"The number samples we have of each type of wine are: \\n\", wine_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41713fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "svc_model = SVC()\n",
    "dt_model = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the models to the training data\n",
    "svc_model.fit(X_train, y_train)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Number of Samples   SVC  Decision Tree\n",
      "Training                  142 0.673          0.649\n",
      "Validation                 36 0.993          0.922\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "# First for the training set\n",
    "\n",
    "# SVC Model training set\n",
    "svc_model_score = cross_validate(svc_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Decision Tree training set\n",
    "dt_model_score = cross_validate(dt_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Number of Samples\": [X_train.shape[0], X_test.shape[0]],\n",
    "    \"SVC\": [svc_model_score['train_score'].mean(), dt_model_score['train_score'].mean()],\n",
    "    \"Decision Tree\": [svc_model_score['test_score'].mean(), dt_model_score['test_score'].mean()]\n",
    "}, index=['Training', 'Validation'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "# TO DO: Print confusion matrix using a heatmap\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "prediction = dt_model.predict(X_test)\n",
    "mat = confusion_matrix(y_test, prediction)\n",
    "\n",
    "sns.heatmap(mat, square = True, annot = True, cbar = False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.88      1.00      0.93        14\n",
      "           3       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.93      0.94        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "\n",
    "print(classification_report(y_test, prediction, target_names=[\"1\", \"2\", \"3\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74067a4",
   "metadata": {},
   "source": [
    "1. The training and validation accuracy changed based on whether the Decision Tree or Support Vector Machine was used. The training accuracy for the decision tree was 64.9% versus 67.3% for SVC. The validation accuracy followed the same pattern, the decision tree validation accuracy was 92.2% versus 99.3%. These accuracy values are negated, so higher values indicate better performance. The SVC model had higher accuracy in both the training and validation sets, thus, making it the better-performing model in this context.\n",
    "\n",
    "2. The support vector machines model did not work as well as the tree-based models because tree-based models work well when there are features with different scales, or a mix of binary and continuous features. SVC models also need to have C and gamma specified, but we have not done either. \n",
    "\n",
    "3. The following were incorrectly classified in Step 5.2:\n",
    "    - Class 1: 1 sample was incorrectly classified\n",
    "    - Class 2: 0 samples were incorrectly classified\n",
    "    - Class 3: 1 sample was incorrectly classified\n",
    "    \n",
    "    Therefore, a total of 2 samples were incorrectly classified.\n",
    "    \n",
    "4. Precision is the ratio of true positive predictions to the total predicted positives. It is important when the cost of false positives is high. Recall is the ratio of true positive predictions to the total actual positives. It is crucial when it is costly to miss positive samples. In this context, recall measures how many of the actual positive samples were correctly identified. Therefore, in the wine dataset example, it is more important to prioritize Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c5efb",
   "metadata": {},
   "source": [
    "1. My code was sourced from the linear regression example that we went over in class along with the regression metrics, decision tree, and SVM examples we did. I also used a lot of the previous assignment to reference how to set up a lot of the same steps that we do every time we do machine learning.\n",
    "\n",
    "2. The steps were completed in the order they were presented in the assignment itself. For parts that I got stuck on I moved on to the second problem and vice versa until I was able to work through all of the problems very sequentially I would say. I also find for machine learning since the five steps are always the same it is just easier to use the same methodology and go from step one to five in order.\n",
    "\n",
    "3. Generative AI was not used for any part of Question Two\n",
    "\n",
    "4. I don't think there were any real challenges, the nice thing with machine learning is that the steps are always the same and if you're following them not much changes besides the interpretation of the models that we will be using but the coding itself is pretty straightforward. We've done it almost three times now and we do it in all the labs so I find that the coding is not the hard part it's the interpretation of the models and the stats from the models are difficult to keep straight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5f1d2",
   "metadata": {},
   "source": [
    "Model Evaluation: In classification, precision, recall, and F1-score provide valuable insights into model performance. The Decision Tree model displayed high precision, recall, and F1-score for each class, indicating its ability to classify wine samples accurately.\n",
    "\n",
    "Model Performance: In the first regression task, the tree-based models (Decision Tree, Random Forest, Gradient Boosting) outperformed the linear regression model. This indicates their ability to capture complex relationships in the data. In the classification task, the Decision Tree model showed better accuracy compared to the Support Vector Machine (SVM) model, highlighting the importance of selecting models that match the dataset's characteristics.\n",
    "\n",
    "Sensitivity to Dataset Characteristics: The choice of the most suitable model depends on the dataset's characteristics. Tree-based models are effective when dealing with non-linear and complex data, while SVMs may perform better on linearly separable data. It's important to consider the nature of the dataset when selecting a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30c25b",
   "metadata": {},
   "source": [
    "The part of the assignment that I really enjoyed was using different models and seeing how they interact with data differently. I disliked also using those multiple models because some of the lines of code started to look the same after a while and it was really hard to differentiate. What I found very interesting about this was being able to use data sets that are seemingly mundane an create interesting models and observations via the models to learn more about what seems like very boring data. I found it challenging working with so many of them at a certain time but it was also very motivating when I finally got codes to run an I could compare what was going on between each model. I also needed a few minutes when my confusion matrix came out as three by three because I was expecting something two by two like the example in class without realizing that I have three classes in the wine data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a308e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/yajurvashisht/opt/anaconda3/envs/ensf-ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'svc_model_train_avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m linear_svc_model_val_avg \u001b[38;5;241m=\u001b[39m linear_svc_val\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mfloat_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat\n\u001b[1;32m     17\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear SVC\u001b[39m\u001b[38;5;124m\"\u001b[39m: [linear_svc_model_train_avg, linear_svc_model_val_avg],\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43msvc_model_train_avg\u001b[49m, svc_model_val_avg]\n\u001b[1;32m     20\u001b[0m }, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svc_model_train_avg' is not defined"
     ]
    }
   ],
   "source": [
    "#ANSWER HERE\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc_model = LinearSVC(max_iter=5000)\n",
    "linear_svc_model.fit(X_train, y_train)\n",
    "\n",
    "linear_svc_train = -cross_val_score(linear_svc_model, X_train, y_train, scoring='accuracy', cv=5)\n",
    "\n",
    "linear_svc_model_train_avg = linear_svc_train.mean()\n",
    "\n",
    "linear_svc_val = -cross_val_score(linear_svc_model, X_test, y_test, scoring='accuracy', cv=5)\n",
    "\n",
    "linear_svc_model_val_avg = linear_svc_val.mean()\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Linear SVC\": [linear_svc_model_train_avg, linear_svc_model_val_avg],\n",
    "    \"Decision Tree\": [svc_model_train_avg, svc_model_val_avg]\n",
    "}, index=['Training', 'Validation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e6c42",
   "metadata": {},
   "source": [
    "The LinearSVC Model does not work well for this dataset as shown above because if the decision boundary that separates different classes in the dataset is highly non-linear, LinearSVC cannot capture the non-linearity effectively. We should use non-linear classifiers like Support Vector Machine with a kernel or decision tree-based models(like done in part two). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209706de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
